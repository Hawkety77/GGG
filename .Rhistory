summary(df$age)
# Bivariate
ggplot(data = df) +
geom_point(aes(x = age, y = dex))
cor(df$dex, df$age)
# Linearity
autoplot(df.lm, which = 1)
# Normality
autoplot(df.lm, which = 2)
shapiro.test(df.lm$residuals)
# Constant Variance
autoplot(df.lm, which = 3)
df.lm <- lm(dex ~ age, data = df)
df$fitted.values <- df.lm$fitted.values
df$residuals <- df.lm$residuals
df <- df %>%
arrange(.$fitted.values) |>
mutate(index = 1:nrow(df),
e_group = ifelse(index < nrow(df)/2, 1, 2) |>
as.factor()) |>
mutate(vals = abs(residuals - median(residuals)), .by = "e_group")
bf.test(vals ~ e_group, df)
# Influential Points
cd_cont_pos <- function(leverage, level, model) {sqrt(level*length(coef(model))*(1-leverage)/leverage)}
cd_cont_neg <- function(leverage, level, model) {-cd_cont_pos(leverage, level, model)}
cd_threshold <- .5
autoplot(df.lm, which = 5) +
stat_function(fun = cd_cont_pos,
args = list(level = cd_threshold, model = df.lm),
xlim = c(0, 0.25), lty = 2, colour = "red") +
stat_function(fun = cd_cont_neg,
args = list(level = cd_threshold, model = df.lm),
xlim = c(0, 0.25), lty = 2, colour = "red") +
scale_y_continuous(limits = c(-4, 4))
ggplot(data = df) +
geom_point(aes(x = age, y = dex)) +
geom_smooth(aes(x = age, y = dex), method = 'lm', se = FALSE)
ggplot(data = df[-2, ]) +
geom_point(aes(x = age, y = dex)) +
geom_smooth(aes(x = age, y = dex), method = 'lm', se = FALSE)
invTranPlot(dex ~ age, data = df, lambda = c(-1, 0, 1), optimal = TRUE)
summary(df.lm)
# <your code here>
# <your code here>
# Fitting the Model
df.lm <- lm(dex ~ age, data = df)
summary(df.lm)$intercept
x <- summary(df.lm)
View(x)
summary(df.lm)$r.squared
anova(df.lm)
View(x)
summary(df.lm)
anova(df.lm)
up <- 3752.5 + qt(.975, 41) * 215.7
low <- 3752.5 - qt(.975, 41) * 215.7
paste0('(', low, ', ', up, ')')
summary(df.lm)
anova(df.lm)
up <- 5.010 + qt(.975, 50) * 1.501
low <- 5.010 - qt(.975, 50) * 1.501
paste0('(', low, ', ', up, ')')
predict(df.lm, data.frame(age = c(22, 48, 92)), interval = 'prediction', level = .95)
mean(df$age)
library(tidyverse)
library(ggfortify)
library(vroom)
library(corrplot)
library(car)
# residual vs. predictor plots
resid_vs_age <- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = age, y = residuals)) +
theme(aspect.ratio = 1)
library(tidyverse)
library(ggfortify)
library(vroom)
library(corrplot)
library(car)
bodyfat <- vroom('BodyFat.txt') %>%
select(-row)
summary(bodyfat)
pairs(bodyfat, pch = 19, lower.panel = NULL)
corrplot(cor(bodyfat), type = "upper")
bodyfat.lm <- lm(brozek ~ ., data = bodyfat)
summary(bodyfat.lm)
bodyfat$residuals <- bodyfat.lm$residuals
# residual vs. predictor plots
resid_vs_age <- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = age, y = residuals)) +
theme(aspect.ratio = 1)
resid_vs_weight <- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = weight, y = residuals)) +
theme(aspect.ratio = 1)
resid_vs_height<- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = height, y = residuals)) +
theme(aspect.ratio = 1)
resid_vs_neck <- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = neck, y = residuals)) +
theme(aspect.ratio = 1)
resid_vs_chest <- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = chest, y = residuals)) +
theme(aspect.ratio = 1)
resid_vs_abdom <- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = abdom, y = residuals)) +
theme(aspect.ratio = 1)
# put plots in 2 rows & 3 columns using the patchwork package
(resid_vs_age | resid_vs_weight | resid_vs_height) /
(resid_vs_neck | resid_vs_chest | resid_vs_abdom)
library(tidyverse)
library(ggfortify)
library(vroom)
library(corrplot)
library(car)
library(patchwork)
library(tidyverse)
library(ggfortify)
library(vroom)
library(corrplot)
library(car)
library(patchwork)
bodyfat <- vroom('BodyFat.txt') %>%
select(-row)
summary(bodyfat)
pairs(bodyfat, pch = 19, lower.panel = NULL)
corrplot(cor(bodyfat), type = "upper")
bodyfat.lm <- lm(brozek ~ ., data = bodyfat)
summary(bodyfat.lm)
bodyfat$residuals <- bodyfat.lm$residuals
# residual vs. predictor plots
resid_vs_age <- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = age, y = residuals)) +
theme(aspect.ratio = 1)
resid_vs_weight <- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = weight, y = residuals)) +
theme(aspect.ratio = 1)
resid_vs_height<- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = height, y = residuals)) +
theme(aspect.ratio = 1)
resid_vs_neck <- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = neck, y = residuals)) +
theme(aspect.ratio = 1)
resid_vs_chest <- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = chest, y = residuals)) +
theme(aspect.ratio = 1)
resid_vs_abdom <- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = abdom, y = residuals)) +
theme(aspect.ratio = 1)
# put plots in 2 rows & 3 columns using the patchwork package
(resid_vs_age | resid_vs_weight | resid_vs_height) /
(resid_vs_neck | resid_vs_chest | resid_vs_abdom)
# partial regression plots
avPlots(bodyfat.lm)
# residual vs fitted values
autoplot(bodyfat.lm, which = 1)
# Diagnostic 1
hist(bodyfat$brozek)
# Diagnostic 2
autoplot(bodyfat.lm, which = 2)
# Diagnostic 3
shapiro.test(bodyfat$residuals)
autoplot(bodyfat.lm, which = 3)
autoplot(bodyfat.lm, which = 1)
# Cook's Distance
plot(bodyfat.lm, which = 5, cook.levels = c(4/249))
vif(bodyfat.lm)
demo()
error.catching
recursion
clear
clear()
library(tidyverse)
library(ggfortify)
library(vroom)
library(corrplot)
library(car)
library(patchwork)
bodyfat <- vroom('BodyFat.txt') %>%
select(-row)
summary(bodyfat)
pairs(bodyfat, pch = 19, lower.panel = NULL)
library(tidyverse)
library(ggfortify)
library(vroom)
library(corrplot)
library(car)
library(patchwork)
bodyfat <- vroom('BodyFat.txt') %>%
select(-row)
summary(bodyfat)
pairs(bodyfat, pch = 19, lower.panel = NULL)
corrplot(cor(bodyfat), type = "upper")
bodyfat.lm <- lm(brozek ~ ., data = bodyfat)
summary(bodyfat.lm)
bodyfat$residuals <- bodyfat.lm$residuals
# residual vs. predictor plots
resid_vs_age <- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = age, y = residuals)) +
theme(aspect.ratio = 1)
resid_vs_weight <- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = weight, y = residuals)) +
theme(aspect.ratio = 1)
resid_vs_height<- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = height, y = residuals)) +
theme(aspect.ratio = 1)
resid_vs_neck <- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = neck, y = residuals)) +
theme(aspect.ratio = 1)
resid_vs_chest <- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = chest, y = residuals)) +
theme(aspect.ratio = 1)
resid_vs_abdom <- ggplot(data = bodyfat) +
geom_point(mapping = aes(x = abdom, y = residuals)) +
theme(aspect.ratio = 1)
# put plots in 2 rows & 3 columns using the patchwork package
(resid_vs_age | resid_vs_weight | resid_vs_height) /
(resid_vs_neck | resid_vs_chest | resid_vs_abdom)
# partial regression plots
avPlots(bodyfat.lm)
# residual vs fitted values
autoplot(bodyfat.lm, which = 1)
# Diagnostic 1
hist(bodyfat$brozek)
# Diagnostic 2
autoplot(bodyfat.lm, which = 2)
# Diagnostic 3
shapiro.test(bodyfat$residuals)
autoplot(bodyfat.lm, which = 3)
autoplot(bodyfat.lm, which = 1)
# Cook's Distance
plot(bodyfat.lm, which = 5, cook.levels = c(4/249))
vif(bodyfat.lm)
# load packages here
library(tidyverse)
library(vroom)
salary <- vroom('Salary.txt') #%>%
View(salary)
salary <- vroom('Salary.txt') %>%
mutate(Education = as.factor(Education)) %>%
mutate(Manager = as.factor(Manager))
```{r, message = FALSE}
salary <- vroom('Salary.txt') %>%
mutate(Education = as.factor(Education)) %>%
mutate(Manager = as.factor(Manager))
summary(salary)
pairs(salary, pch = 19, lower.panel = NULL)
pairs(salary, pch = 11, lower.panel = NULL)
pairs(salary, pch = 19, lower.panel = NULL)
ggplot(aes(x = Experience, y = Salary)) +
geom_scatter()
library(ggfortify)
ggplot(aes(x = Experience, y = Salary)) +
geom_scatter()
ggplot(data = salary, aes(x = Experience, y = Salary)) +
geom_scatter()
ggplot(data = salary, aes(x = Experience, y = Salary)) +
geom_point()
ggplot(data = salary, aes(x = Experience, y = Salary)) +
geom_boxplot()
ggplot(data = salary, aes(x = Manager, y = Salary)) +
geom_boxplot()
ggplot(data = salary, aes(x = Experience, y = Salary, color = Education, shape = Manager)) +
geom_point()
# Either one of these will do:
# interaction plot colored by Manager
interaction.plot(x.factor = salary$Education,
trace.factor = salary$Manager,
response = salary$Salary,
col = c("#1b9e77", "#d95f02"),
lwd = 2,
trace.label = "Manager",
ylab = "Average Quarterly Salary ($)",
xlab = "Education")
# interaction plot colored by Education
interaction.plot(x.factor = salary$Manager,
trace.factor = salary$Education,
response = salary$Salary,
col = c("#1b9e77", "#d95f02", "#7570b3"),
lwd = 2,
trace.label = "Education",
ylab = "Average Quarterly Salary ($)",
xlab = "Manager")
salary_lm_initial <- lm(Salary ~ ., data = salary)
summary(salary_lm_initial)
# You can do this like this:
levels(salary$Education)  # order of levels originally
salary$Education <- factor(salary$Education, levels = c("HS", "BS", "BS+"))
# Or like this:
salary <- salary |>
mutate(Education = fct_relevel(Education, "HS"))
salary.lm <- lm(Salary ~ ., data = salary)
salary.lm
salary$EducationHS <- ifelse(salary$Education == "HS", 1, 0)
salary$EducationBS <- ifelse(salary$Education == "BS", 1, 0)
salary$EducationBSplus <- ifelse(salary$Education == "BS+", 1, 0)
# your code continues here
salary$EducationHS <- ifelse(salary$Education == "HS", 1, 0)
salary$EducationBS <- ifelse(salary$Education == "BS", 1, 0)
salary$EducationBSplus <- ifelse(salary$Education == "BS+", 1, 0)
salary$ManagerYes <- ifelse(salary$Manager == "Yes", 1, 0)
salary$ManagerNo <- ifelse(salary$Manager == "No", 1, 0)
# your code continues here
salary.lm <- lm(Salary ~ Experience + EducationHS + EducationBS + EducationBSplus + ManagerNo + ManagerYes, data = salary)
salary.lm
setwd("~/School Projects/GGG")
library(tidymodels)
library(tidyverse)
library(embed)
library(vroom)
library(doParallel)
library(themis)
library(naivebayes)
library(discrim)
detectCores() #How many cores do I have?
cl <- makePSOCKcluster(7)
registerDoParallel(cl)
df_train <- vroom('train.csv')
df_test <- vroom('test.csv')
my_recipe <- recipe(type ~ ., data = df_train) %>%
step_mutate(color = as.factor(color)) %>%
step_lencode_glm(all_nominal_predictors(), outcome = vars(type)) %>%
step_normalize()
prep <- prep(my_recipe)
baked <- bake(prep, new_data = NULL)
my_model <- naive_Bayes(Laplace=tune(), smoothness=tune()) %>% # set or tune
set_mode("classification") %>%
set_engine("naivebayes")
am_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_model)
tuning_grid <- grid_regular(Laplace(),
smoothness(),
levels = 5)
folds <- vfold_cv(df_train, v = 5, repeats = 1)
tree_tune <- tune_grid(my_model,
my_recipe,
folds,
control = control_grid(save_workflow = TRUE),
grid = tuning_grid)
collect_metrics(tree_tune)
show_best(tree_tune, metric = 'accuracy')
best_tune <- tree_tune %>%
select_best("accuracy")
final_workflow <- rand_workflow %>%
finalize_workflow(best_tune) %>%
fit(data = df_train)
final_workflow <- am_workflow %>%
finalize_workflow(best_tune) %>%
fit(data = df_train)
ggg_predictions <- predict(final_workflow,
new_data = df_test,
type = 'class')
submission <- ggg_predictions %>%
bind_cols(df_test) %>%
select(id, .pred_class) %>%
rename(type = .pred_class)
write_csv(submission, 'submission_naivebayes.csv')
stopCluster(cl)
library(tidymodels)
library(tidyverse)
library(embed)
library(vroom)
library(doParallel)
library(themis)
library(naivebayes)
library(discrim)
detectCores() #How many cores do I have?
cl <- makePSOCKcluster(7)
registerDoParallel(cl)
df_train <- vroom('train.csv')
df_test <- vroom('test.csv')
my_recipe <- recipe(type ~ ., data = df_train) %>%
step_rm(id) %>%
step_mutate(color = as.factor(color)) %>%
step_lencode_glm(all_nominal_predictors(), outcome = vars(type)) %>%
step_normalize()
prep <- prep(my_recipe)
baked <- bake(prep, new_data = NULL)
my_model <- naive_Bayes(Laplace=tune(), smoothness=tune()) %>% # set or tune
set_mode("classification") %>%
set_engine("naivebayes")
am_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_model)
tuning_grid <- grid_regular(Laplace(),
smoothness(),
levels = 5)
folds <- vfold_cv(df_train, v = 5, repeats = 1)
tree_tune <- tune_grid(my_model,
my_recipe,
folds,
control = control_grid(save_workflow = TRUE),
grid = tuning_grid)
collect_metrics(tree_tune)
show_best(tree_tune, metric = 'accuracy')
best_tune <- tree_tune %>%
select_best("accuracy")
final_workflow <- am_workflow %>%
finalize_workflow(best_tune) %>%
fit(data = df_train)
#########
ggg_predictions <- predict(final_workflow,
new_data = df_test,
type = 'class')
submission <- ggg_predictions %>%
bind_cols(df_test) %>%
select(id, .pred_class) %>%
rename(type = .pred_class)
write_csv(submission, 'submission_naivebayes.csv')
stopCluster(cl)
library(tidymodels)
library(tidyverse)
library(embed)
library(vroom)
library(doParallel)
library(themis)
library(naivebayes)
library(discrim)
detectCores() #How many cores do I have?
cl <- makePSOCKcluster(7)
registerDoParallel(cl)
df_train <- vroom('train.csv')
df_test <- vroom('test.csv')
my_recipe <- recipe(type ~ ., data = df_train) %>%
step_rm(id) %>%
step_mutate(color = as.factor(color)) %>%
step_lencode_glm(all_nominal_predictors(), outcome = vars(type)) %>%
step_normalize()
prep <- prep(my_recipe)
baked <- bake(prep, new_data = NULL)
my_model <- naive_Bayes(Laplace=tune(), smoothness=tune()) %>% # set or tune
set_mode("classification") %>%
set_engine("naivebayes")
am_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_model)
tuning_grid <- grid_regular(Laplace(),
smoothness(),
levels = 5)
folds <- vfold_cv(df_train, v = 5, repeats = 1)
tree_tune <- tune_grid(my_model,
my_recipe,
folds,
control = control_grid(save_workflow = TRUE),
grid = tuning_grid)
collect_metrics(tree_tune)
show_best(tree_tune, metric = 'accuracy')
best_tune <- tree_tune %>%
select_best("accuracy")
final_workflow <- am_workflow %>%
finalize_workflow(best_tune) %>%
fit(data = df_train)
#########
ggg_predictions <- predict(final_workflow,
new_data = df_test,
type = 'class')
submission <- ggg_predictions %>%
bind_cols(df_test) %>%
select(id, .pred_class) %>%
rename(type = .pred_class)
write_csv(submission, 'submission_naivebayes.csv')
stopCluster(cl)
library(tidymodels)
library(tidyverse)
library(embed)
library(vroom)
library(doParallel)
library(themis)
library(naivebayes)
library(discrim)
detectCores() #How many cores do I have?
cl <- makePSOCKcluster(7)
registerDoParallel(cl)
df_train <- vroom('train.csv')
df_test <- vroom('test.csv')
my_recipe <- recipe(type ~ ., data = df_train) %>%
step_mutate(color = as.factor(color)) %>%
step_lencode_glm(all_nominal_predictors(), outcome = vars(type)) #%>%
# step_normalize()
prep <- prep(my_recipe)
baked <- bake(prep, new_data = NULL)
my_model <- naive_Bayes(Laplace=tune(), smoothness=tune()) %>% # set or tune
set_mode("classification") %>%
set_engine("naivebayes")
am_workflow <- workflow() %>%
add_recipe(my_recipe) %>%
add_model(my_model)
tuning_grid <- grid_regular(Laplace(),
smoothness(),
levels = 5)
folds <- vfold_cv(df_train, v = 5, repeats = 1)
tree_tune <- tune_grid(my_model,
my_recipe,
folds,
control = control_grid(save_workflow = TRUE),
grid = tuning_grid)
collect_metrics(tree_tune)
show_best(tree_tune, metric = 'accuracy')
best_tune <- tree_tune %>%
select_best("accuracy")
final_workflow <- am_workflow %>%
finalize_workflow(best_tune) %>%
fit(data = df_train)
#########
ggg_predictions <- predict(final_workflow,
new_data = df_test,
type = 'class')
submission <- ggg_predictions %>%
bind_cols(df_test) %>%
select(id, .pred_class) %>%
rename(type = .pred_class)
write_csv(submission, 'submission_naivebayes.csv')
stopCluster(cl)
